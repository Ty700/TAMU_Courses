
Pipelining, which overlaps instruction execution to take advantage of instruction-level parallelism and minimize delays, was covered in Chapter 3. Data risks brought on by dependencies like RAW, WAW, and WAR, however, can still happen. These risks are mitigated by methods such as register renaming and loop-level parallelism. Instruction order is also impacted by control dependencies, which reduces the possibility of parallel execution.

Dynamic branch prediction, which operates by identifying recurring patterns in data, algorithms, and instruction sequences, was also explored in this chapter. It performs better than static prediction, especially for dynamic branching. A 2-bit method might assist minimize mispredictions, particularly in loops, although hardware constraints may cause branch history table indexing issues.
There was also discussion of correlated branch predictors, which increase prediction accuracy by forecasting future branches based on past performance. In history tables, branch outcomes are tracked by a 2-bit predictor; more intricate relationships are captured by bigger (m,n) schemes. When dealing with intricate benchmarks that have connected branches, this strategy is quite helpful.

The chapter described how several prediction techniques are combined by tournament predictors, and how an n-bit saturation counter is used to choose the most accurate predictor. Both global and local histories are used by systems such as ARPA, with tournament predictors choosing the optimal choice for different branch behaviors. Lastly, more sophisticated models outperform conventional gshare predictors by employing several tables to decrease hash collisions and increase prediction accuracy. These models are known as Tagged Hybrid Predictors.